

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>19.5 Learned Approximate Inference &mdash; dl 0.0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Paper: Auto-Encoding Variational Bayes" href="Auto Encoding Variational Bayes.html" />
    <link rel="prev" title="19.4 Variational Inference and Learning" href="19.4 Variational Inference and Learning.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> dl
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../Part 1 (Applied Math and Machine Learning Basics)/index.html">Part I: Applied Math and Machine Learning Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Part 2 (Modern Practical Deep Networks)/index.html">Part II: Modern Practical Deep Networks</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Part III: Deep Learning Research</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../14 Autoencoders/index.html">14 Autoencoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="../15 Representation Learning/index.html">15 Representation Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../16 Structured Probablistic Models for Deep Learning/index.html">16 Structured Probablistic Models for Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../17 Monte Carlo Methods/index.html">17 Monte Carlo Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../18 Confronting the Partition Function/index.html">18 Confronting the Partition Function</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">19 Approximate Inference</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="19.1 Inference as Optimization.html">19.1 Inference as Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="19.2 Expectation Maximization.html">19.2 Expectation Maximization</a></li>
<li class="toctree-l3"><a class="reference internal" href="19.3 MAP Inference and Sparse Coding.html">19.3 MAP Inference and Sparse Coding</a></li>
<li class="toctree-l3"><a class="reference internal" href="19.4 Variational Inference and Learning.html">19.4 Variational Inference and Learning</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">19.5 Learned Approximate Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#wake-sleep">19.5.1 Wake-Sleep</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="Auto Encoding Variational Bayes.html">Paper: Auto-Encoding Variational Bayes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../20 Deep Generative Models/index.html">20 Deep Generative Models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../Extra/index.html">Extra</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">dl</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Part III: Deep Learning Research</a> &raquo;</li>
        
          <li><a href="index.html">19 Approximate Inference</a> &raquo;</li>
        
      <li>19.5 Learned Approximate Inference</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/Part 3 (Deep Learning Research)/19 Approximate Inference/19.5 Learned Approximate Inference.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="learned-approximate-inference">
<h1>19.5 Learned Approximate Inference<a class="headerlink" href="#learned-approximate-inference" title="Permalink to this headline">¶</a></h1>
<p>Explicitly performing optimization via</p>
<ul class="simple">
<li>fixed point equition</li>
<li>gradient-based optimization</li>
</ul>
<p>is often very expensive and time consuming.</p>
<p>Solution: we can think of the optimzation process as a function f that maps an input v to an approximate distribution <span class="math notranslate nohighlight">\(q^* = argmax_q L(v, q)\)</span>. Once we think of the multistep iterative optimization process as just being a function, we can approximate it with a neural network that implements an approaximate <span class="math notranslate nohighlight">\(\hat{f}(v, \theta)\)</span></p>
<div class="section" id="wake-sleep">
<h2>19.5.1 Wake-Sleep<a class="headerlink" href="#wake-sleep" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>The main difficulties with training a model to infer h from v is that we do not have a supervised training set with which to train the model. Given a v, we do not know the appropriate h.</li>
<li>Solution: the wake-sleep algorithm resolves this problem by drawing samples of both h and v from the model distribution.</li>
<li>Drawback of this approach: we will only by able to train the inference network on values of v that have high probability under the model. Early in learning, the model distribution will not resemble the data distribution, so the inference network will not have an opportunity to learn on samples that resemble data.</li>
</ul>
<p>Review on 18.1:</p>
<p>Target probability distribution:</p>
<div class="math notranslate nohighlight">
\[p(x; \theta) = \frac{1}{Z(\theta)}\hat{p}(x; \theta)\]</div>
<p>The gradient</p>
<div class="math notranslate nohighlight">
\[\nabla_{\theta} = \nabla_{\theta} \log\hat{p}(x;\theta) - \nabla_{\theta} \log Z(\theta)\]</div>
<ul class="simple">
<li><span class="math notranslate nohighlight">\(\nabla_{\theta} \log\hat{p}(x;\theta)\)</span> : Positive phase. Models with no latent variables or with few interaction between the latent variables typically have a tractable positive phase.</li>
<li><span class="math notranslate nohighlight">\(\nabla_{\theta} \log Z(\theta)\)</span>: Negative phase. For undirected model, the negative phase is difficult.</li>
</ul>
<p>Review on 18.2</p>
<p><strong>Because the negative phase involves drawing samples from the model’s distribution, we can think of it as ﬁnding points that the model believes in strongly. Because the negative phase acts to reduce the probability of those points, they are generally considered to represent the model’s incorrect beliefs about the world. They are frequently referred to in the literature as “hallucinations” or “fantasyparticles.” In fact, the negative phase has been proposed as a possible explanation for dreaming in humans and other animals, the idea being that the brain maintains a probabilistic model of the world and follows the gradient of</strong> <span class="math notranslate nohighlight">\(\log \hat{p}\)</span> <strong>when experiencing real events while awake and follows the negative gradient of</strong> <span class="math notranslate nohighlight">\(\log \hat{p}\)</span> <strong>to minimize log Z while sleeping and experiencing events sampled from the current model.</strong></p>
<p>Another possible explanation for biological dreaming is that it is providing samples from p(h, v) which can be used to train an inference network to predict h given v.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Auto Encoding Variational Bayes.html" class="btn btn-neutral float-right" title="Paper: Auto-Encoding Variational Bayes" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="19.4 Variational Inference and Learning.html" class="btn btn-neutral" title="19.4 Variational Inference and Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Ximing

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>