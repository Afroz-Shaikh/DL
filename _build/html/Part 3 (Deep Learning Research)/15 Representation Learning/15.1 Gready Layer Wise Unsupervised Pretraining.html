

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>15.1 Gready Layer-Wise Unsupervised Pretraining &mdash; dl 0.0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="15.2 Transfer Learning and Domain Adaptation" href="15.2 Transfer Learning and Domain Adaptation.html" />
    <link rel="prev" title="15 Representation Learning" href="index.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> dl
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../Part 1 (Applied Math and Machine Learning Basics)/index.html">Part I: Applied Math and Machine Learning Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Part 2 (Modern Practical Deep Networks)/index.html">Part II: Modern Practical Deep Networks</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Part III: Deep Learning Research</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../14 Autoencoders/index.html">14 Autoencoders</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">15 Representation Learning</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">15.1 Gready Layer-Wise Unsupervised Pretraining</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#when-and-why-does-unsupervised-pretraining-work">When and why does unsupervised pretraining work</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="15.2 Transfer Learning and Domain Adaptation.html">15.2 Transfer Learning and Domain Adaptation</a></li>
<li class="toctree-l3"><a class="reference internal" href="15.3 Semi Supervised Disentangling of Casual Factors.html">15.3 Semi-Supervised Disentangling of Casual Factors</a></li>
<li class="toctree-l3"><a class="reference internal" href="15.4 Distributed Representation.html">15.4 Distributed Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="15.5 Exponential Gains from Depth.html">15.5 Exponential Gains from Depth</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../Extra/index.html">Extra</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">dl</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Part III: Deep Learning Research</a> &raquo;</li>
        
          <li><a href="index.html">15 Representation Learning</a> &raquo;</li>
        
      <li>15.1 Gready Layer-Wise Unsupervised Pretraining</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/Part 3 (Deep Learning Research)/15 Representation Learning/15.1 Gready Layer Wise Unsupervised Pretraining.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="gready-layer-wise-unsupervised-pretraining">
<h1>15.1 Gready Layer-Wise Unsupervised Pretraining<a class="headerlink" href="#gready-layer-wise-unsupervised-pretraining" title="Permalink to this headline">¶</a></h1>
<p>Greedy Layer-Wise Unsupervised Pretraining relies on single-layer representation learning algorithm. Each layer is pretrained using unsupervised learning, taking the output of previous layer and producing as output a new representation of the data, whose distribution is hopefully simpler.</p>
<img alt="../../_images/Algorithm15.1.PNG" src="../../_images/Algorithm15.1.PNG" />
<p>Greedy layer-wise unsupervsied pretraining name explanation:</p>
<ul>
<li><p class="first">Gready: Optimize each piece of the solution independently, on piece at a time.</p>
</li>
<li><p class="first">Layer-Wise: The independent pieces are the layer of the network. Training proceeds once layer at a time, training the k-th layer while keeping the previous ones fixed.</p>
</li>
<li><p class="first">Unsupervised: each layer is trained with an unsupervised representation learning algorithm.</p>
</li>
<li><p class="first">Pretraining:</p>
<blockquote>
<div><ul class="simple">
<li>1st step/phase: Greedy Layer-Wise Unsupervised Pretraining</li>
<li>Fine-Tune all the layers together</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>In the context of supervised learning, it could be viewed as</p>
<ul class="simple">
<li>Regulirizer</li>
<li>A form a parameter initialization</li>
</ul>
<p>Greedy Layer-Wise unsupervised pretraining can also be used as initialization for other unsupervised learning algorithms such as</p>
<ul>
<li><p class="first">Deep Autoencoders</p>
</li>
<li><p class="first">Probabilistic models with many layers of latent variables, including:</p>
<blockquote>
<div><ul class="simple">
<li>Deep Belief Network</li>
<li>Deep Boltzman Machines</li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="section" id="when-and-why-does-unsupervised-pretraining-work">
<h2>When and why does unsupervised pretraining work<a class="headerlink" href="#when-and-why-does-unsupervised-pretraining-work" title="Permalink to this headline">¶</a></h2>
<p>Unsupervised pretrining combines 2 ideas:</p>
<div class="section" id="idea1-choice-of-initial-parameters-for-a-deep-nn-can-have-a-significant-regularizing-effect-on-the-model">
<h3>Idea1: Choice of initial parameters for a deep NN can have a significant regularizing effect on the model<a class="headerlink" href="#idea1-choice-of-initial-parameters-for-a-deep-nn-can-have-a-significant-regularizing-effect-on-the-model" title="Permalink to this headline">¶</a></h3>
<p>It remains possible that pretraining initializes the model in a location that would otherwise be inaccessible. Eg:</p>
<blockquote>
<div><ol class="arabic simple">
<li>a region surrounded by areas where the cost function varies so much from one example to another that minibatches give only a very noisy estimate of the gradient</li>
<li>a region surrounded by areas where the Hessian matrix is so pooly conditioned that gradient descent methods must use very small steps.</li>
</ol>
</div></blockquote>
<p>We cannot charactorize the exactly what aspects of the pretrained parameters are retained during the supervised training stages. This is one of the reason why modern approaches typically use simultaneous unsupervised learning and supervised learning rather than two sequential stages.</p>
<p>You may also freeze the parameters for the feature extractor and use supervised learning only to add a classifier on top of the learned features.</p>
</div>
<div class="section" id="idea2-learning-about-the-input-distribution-can-help-with-learning-about-the-mapping-from-input-to-output">
<h3>Idea2: Learning about the input distribution can help with learning about the mapping from input to output.<a class="headerlink" href="#idea2-learning-about-the-input-distribution-can-help-with-learning-about-the-mapping-from-input-to-output" title="Permalink to this headline">¶</a></h3>
<p>Some features that are useful for the unsupervised task may also be useful for supervised task. This is not yet understood at a mathmatical, theoretical level. Many aspects of this approach are highly dependent on the specific models used. Eg: if we wish to add a linear classifer on top of pretrained features, the featrues mush make the underlyin classes linearly seperable. This is another reason that simultaneous supervised and unsupervised learning can be preferable.</p>
</div>
<div class="section" id="interpretation-to-the-function-of-unsupervised-training">
<h3>Interpretation to the function of unsupervised training<a class="headerlink" href="#interpretation-to-the-function-of-unsupervised-training" title="Permalink to this headline">¶</a></h3>
<p>Unsupervised training as learning a representation: We can expect unsupervised pretraining to be more effective when the initial representation is poor. Eg: the use of word embedding. It is less useful for image processing perhaps because images lie in a rich vector space where distance provide a low quality similarity metrics.</p>
<p>Unsupervised training as a regularizer: We can also expect unsupervised pretraining to be most helpful when the number of labeled examples is very small.</p>
<p>Unsupervised pretraining is likely to be most useful when the function to be learned is extremely complicated.</p>
</div>
<div class="section" id="success-case-of-unsupervised-pretraining">
<h3>Success case of Unsupervised Pretraining<a class="headerlink" href="#success-case-of-unsupervised-pretraining" title="Permalink to this headline">¶</a></h3>
<p>Unsupervised pretraining has usually been used to improve classifier is usually interesting from the point of view of reducing test set error. It can help tasks other than classification.</p>
<p>Improvement to training error and test error maybe both explained in terms of unsupervised pretraining taking training into a region that would otherwise be inaccessible.</p>
<ul class="simple">
<li>NN with unsupervised pretraining: halt in the same smaller region of function space. Suggesting that pretraining reduces the variance of function estimation process =&gt; reduce the risk of severe over fitting.</li>
<li>NN wihout unsupervised pretraining: halt in another region.</li>
</ul>
<img alt="../../_images/Figure15.1.PNG" src="../../_images/Figure15.1.PNG" />
<p>In another words: unsupervised pretraining initialize NN parameters into a region that they do not escape, and the results following this initializations are more consistent and less likely to be very bad than without pretraining.</p>
</div>
<div class="section" id="disadvantage-of-unsupervised-pretraining">
<h3>Disadvantage of Unsupervised Pretraining<a class="headerlink" href="#disadvantage-of-unsupervised-pretraining" title="Permalink to this headline">¶</a></h3>
<p>Operating with two seperate training phase.</p>
<ol class="arabic simple">
<li>Unsupervsied pretraining does not offer a clear way to adjust the strength of the regularization arising from the unsupervised stage. When we perform unsupervised and supervised learning simultaneously, instead of using the pretraining strategy, there is a single hyperparameter, usually a coefficient attatched to the unsupervised cost, that determine hows strongly supervsied objective will regularize the supervsied model.</li>
<li>Two seperate training phases has its own hyperparameters. The performance of the second phase cannot be predicted during the first phase, so there is a long delay between proposing hyperparameters for the first phase and being able to update them using feedback from the second phase. Most principled approach: use validation set error in the supervised phase to select hyperparameters of the pretraining phase.</li>
</ol>
<p>Today, unsupervised pretraining has been largely abandoned, except for natural language processing.</p>
<p>Deep learning techniques based on supervised learning, regularized with dropout or batch normalization, are able to achieve human level performance on many tasks, but onlu with extremely large labeled datasets. On extremely small datasets, such as the alternative splicing dataset, Bayesian methods outperform methods based on unsupervised learning.</p>
<p>The idea of pretraining has been generalized to supervised pretraining, as a common approach for transfer learning.</p>
<p>Supervised pretraining for transfer learning is popular for use with convolutional networks pretrained on ImageNet dataset. Practitioners publish the parameters of the trained networks for this purpose, just as pretrained word vectors are published for natural language processing tasks.</p>
<p>Review on 8.7.4 Supervised pretraining:</p>
<img alt="../../_images/Figure8.7.PNG" src="../../_images/Figure8.7.PNG" />
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="15.2 Transfer Learning and Domain Adaptation.html" class="btn btn-neutral float-right" title="15.2 Transfer Learning and Domain Adaptation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral" title="15 Representation Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Ximing

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>