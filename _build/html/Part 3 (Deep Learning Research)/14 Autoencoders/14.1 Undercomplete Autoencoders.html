

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>14.1 Undercomplete Autoencoders &mdash; dl 0.0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="14.3 Representational Power, Layer Size and Depth" href="14.3 Representational Power, Layer Size and Depth.html" />
    <link rel="prev" title="14 Autoencoders" href="index.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> dl
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../Part 1 (Applied Math and Machine Learning Basics)/index.html">Part I: Applied Math and Machine Learning Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Part 2 (Modern Practical Deep Networks)/index.html">Part II: Modern Practical Deep Networks</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Part III: Deep Learning Research</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">14 Autoencoders</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">14.1 Undercomplete Autoencoders</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#review-on-info-theory">Review on info theory</a></li>
<li class="toctree-l4"><a class="reference internal" href="#variational-inference">Variational inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="14.3 Representational Power, Layer Size and Depth.html">14.3 Representational Power, Layer Size and Depth</a></li>
<li class="toctree-l3"><a class="reference internal" href="14.4 Stochastic Encoders and Decoders.html">14.4 Stochastic Encoders and Decoders</a></li>
<li class="toctree-l3"><a class="reference internal" href="14.5 Denoising Autoencoders.html">14.5 Denoising Autoencoders</a></li>
<li class="toctree-l3"><a class="reference internal" href="14.6 Learning Manifolds with Autoencoder.html">14.6 Learning Manifolds with Autoencoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="14.7 Contractive Autoencoders.html">14.7 Contractive Autoencoders</a></li>
<li class="toctree-l3"><a class="reference internal" href="14.8 Predictive Sparse Decomposition.html">14.8 Predictive Sparse Decomposition</a></li>
<li class="toctree-l3"><a class="reference internal" href="14.9 Applications of Autoencoders.html">14.9 Applications of Autoencoders</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../15 Representation Learning/index.html">15 Representation Learning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../Extra/index.html">Extra</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">dl</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Part III: Deep Learning Research</a> &raquo;</li>
        
          <li><a href="index.html">14 Autoencoders</a> &raquo;</li>
        
      <li>14.1 Undercomplete Autoencoders</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/Part 3 (Deep Learning Research)/14 Autoencoders/14.1 Undercomplete Autoencoders.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="undercomplete-autoencoders">
<h1>14.1 Undercomplete Autoencoders<a class="headerlink" href="#undercomplete-autoencoders" title="Permalink to this headline">¶</a></h1>
<p>An autoencoder whose code dimension is less than the input dimension is called <strong>undercomplete</strong>.</p>
<p>The learning process: minimizing a loss function</p>
<div class="math notranslate nohighlight">
\[L(x, g(f(x)))\]</div>
<p>where L is a loss function penalizingg g(f(x)) for being dissimilar from x, such as the mean squared error.</p>
<p>When the decoder is linear and L is the mean squared error, an undercomplete autoencoder learns to span the same subspace as PCA. In this case, an autoencoder trained to perform the copying task has learned the principal subspace of the training data as a side eﬀect.</p>
<p>In PCA, we apply a transform of higer dimension data <span class="math notranslate nohighlight">\(x \in R^d\)</span> with <span class="math notranslate nohighlight">\(U \in R^{(d*p)}\)</span> where <span class="math notranslate nohighlight">\(d &gt; p\)</span>:</p>
<div class="math notranslate nohighlight">
\[y = U^Tx\]</div>
<p>To reconstruct the orginal data you can apply:</p>
<div class="math notranslate nohighlight">
\[x = Uy\]</div>
<p>You can imagin an simple autoencoder with applying <span class="math notranslate nohighlight">\(U^T\)</span> as encoder and applying <span class="math notranslate nohighlight">\(U\)</span> as decoder. That is the same logic as PCA</p>
<div class="section" id="review-on-info-theory">
<h2>Review on info theory<a class="headerlink" href="#review-on-info-theory" title="Permalink to this headline">¶</a></h2>
<p>Low possibility: more informational. High possibility: less informational.</p>
<p>Information gain:</p>
<div class="math notranslate nohighlight">
\[I = -log P(x)\]</div>
<p>Entropy (expected information gain)</p>
<div class="math notranslate nohighlight">
\[H = - \sum P(x)log P(x)\]</div>
<p>KL divergence, which measure the similarity of two distribution:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
        KL(P || Q) = - \sum P(x)log Q(x) + \sum P(x)logP(x) \\
        = \sum P(x) log \frac{P(x)}{Q(x)}
\end{equation}\end{split}\]</div>
<p>KL divergence is not . The KL divergence above is of Q with respect to P. There are two important properties of KL divergence:</p>
<ol class="arabic simple">
<li><span class="math notranslate nohighlight">\(KL &gt;= 0\)</span></li>
<li><span class="math notranslate nohighlight">\(KL(P||Q) \neq KL (Q||P)\)</span> (not symmetric)</li>
</ol>
</div>
<div class="section" id="variational-inference">
<h2>Variational inference<a class="headerlink" href="#variational-inference" title="Permalink to this headline">¶</a></h2>
<p>Suppose x is observation and z is hidden variable. In order to compute the relationship between z and x, we have to compute:</p>
<div class="math notranslate nohighlight">
\[\begin{equation}
        P(x|z) = \frac {P(z|x)p(z)}{p(x)}
\end{equation}\]</div>
<p>Computing <span class="math notranslate nohighlight">\(P(x) = \int P(x|z)p(z)dz\)</span> is very challenging (imagin z with very high dimemsion). Sometime it is intractable. There are two ways to deal with this challenge.</p>
<ul class="simple">
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte Carlo Method</a> (not focused here)</li>
<li>Variational Inference (explained below)</li>
</ul>
<p>We can approach P(z|x) with Q(z). If we choose Q to be tractabke distribution, such as Gaussian or Bernoulli, then we have an approximate distribution that we can compute. So the goal is minimize:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
        KL (q(z) || p(z|x)) \\
        &amp; = - \sum q(z) log \frac {p(z|x)}{q(z)} &amp;\\
        &amp; = - \sum q(z) log \frac{\frac {p(x, z)}{p(x)}} {q(z)} &amp;\\
        &amp; = - \sum q(z) (log \frac {p(x, z)}{q(z)} - log p(x) ) &amp;\\
        &amp; = - \sum q(z) log \frac {p(x, z)}{q(z)} + \sum q(z)log p(x) &amp;\\
        &amp; = - \sum q(z) log \frac {p(x, z)}{q(z)} + log p(x) \sum q(z) &amp;\\
        &amp; = - \sum q(z) log \frac {p(x, z)}{q(z)} + log p(x)
\end{aligned}\end{split}\]</div>
<p>So we can rewrite the equation as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
        log p(x) \\
        = KL (q(z) || p(z|x))  + \sum q(z) log \frac {p(x, z)}{q(z)}  \\
\end{equation}\end{split}\]</div>
<p>logP(x) is given, if we want to minimize <span class="math notranslate nohighlight">\(KL (q(z) || p(z|x))\)</span>, we should maximize <span class="math notranslate nohighlight">\(\sum q(z) log \frac {p(x, z)}{q(z)}\)</span> (we can call it L). This term L is also called <strong>variational lower bound</strong>. Since KL divergence is always non-negative, logP(x) is always greater or equal to L.</p>
<div class="math notranslate nohighlight">
\[\begin{split}    \begin{aligned}
\sum q(z) log \frac {p(x, z)}{q(z)} &amp;= \sum q(z)log p(x|z) + \sum q(z)log\frac {p(z)}{q(z)} &amp;\\
                                                            &amp;= \sum q(z)log p(x|z) - KL (q(z) || p(z)) &amp;\\
                                                            &amp;=  - KL (q(z) || p(z))
    \end{aligned}\end{split}\]</div>
<p>To maximize L we need to make <span class="math notranslate nohighlight">\(E_{q(z)} log p(x|z)\)</span> as great as possible.
We can think of <span class="math notranslate nohighlight">\(E_{q(z)} log p(x|z)\)</span> as reconstruction gain. If x is a Gaussian distribution, maximizing <span class="math notranslate nohighlight">\(E_{q(z)} log p(x|z)\)</span> is the same as minimizing <span class="math notranslate nohighlight">\(|x - \hat{x}|^2\)</span>. If x is a Bernoulli, it is the same as cross entropy.</p>
<p>q(z) as similar to p(z) as possible and make</p>
</div>
<div class="section" id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="https://www.youtube.com/watch?v=uaaqyVS9-rM">Ali Ghodsi Variational Autoencoder</a></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="14.3 Representational Power, Layer Size and Depth.html" class="btn btn-neutral float-right" title="14.3 Representational Power, Layer Size and Depth" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral" title="14 Autoencoders" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Ximing

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>