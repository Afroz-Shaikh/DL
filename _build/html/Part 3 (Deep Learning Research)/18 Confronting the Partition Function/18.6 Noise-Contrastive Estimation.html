

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>18.6 Noise-Contrastive Estimation &mdash; dl 0.0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Extra" href="../../Extra/index.html" />
    <link rel="prev" title="18.5 Denoising Score Matching" href="18.5 Denoising Score Matching.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> dl
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../Part 1 (Applied Math and Machine Learning Basics)/index.html">Part I: Applied Math and Machine Learning Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Part 2 (Modern Practical Deep Networks)/index.html">Part II: Modern Practical Deep Networks</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Part III: Deep Learning Research</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../14 Autoencoders/index.html">14 Autoencoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="../15 Representation Learning/index.html">15 Representation Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../16 Structured Probablistic Models for Deep Learning/index.html">16 Structured Probablistic Models for Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../17 Monte Carlo Methods/index.html">17 Monte Carlo Methods</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">18 Confronting the Partition Function</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="18.1 The log-likehood Gradient.html">18.1 The log-likehood Gradient</a></li>
<li class="toctree-l3"><a class="reference internal" href="18.2 Stochastic Maximum Likehood and Contrastive.html">18.2 Stochastic Maximum Likehood and Contrastive Divergence</a></li>
<li class="toctree-l3"><a class="reference internal" href="18.3 Pseudolikehood.html">18.3 Pseudolikehood</a></li>
<li class="toctree-l3"><a class="reference internal" href="18.4 Score Matching and Ratio Matching.html">18.4 Score Matching and Ratio Matching</a></li>
<li class="toctree-l3"><a class="reference internal" href="18.5 Denoising Score Matching.html">18.5 Denoising Score Matching</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">18.6 Noise-Contrastive Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#resource">Resource</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../Extra/index.html">Extra</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">dl</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Part III: Deep Learning Research</a> &raquo;</li>
        
          <li><a href="index.html">18 Confronting the Partition Function</a> &raquo;</li>
        
      <li>18.6 Noise-Contrastive Estimation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/Part 3 (Deep Learning Research)/18 Confronting the Partition Function/18.6 Noise-Contrastive Estimation.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="noise-contrastive-estimation">
<h1>18.6 Noise-Contrastive Estimation<a class="headerlink" href="#noise-contrastive-estimation" title="Permalink to this headline">Â¶</a></h1>
<p>Noise-contrastive estimation estimates probability distribution by</p>
<div class="math notranslate nohighlight">
\[\log_{model}(x) = \log \hat{p}(x; \theta) + c\]</div>
<p>where c is explicitly introduced as an approximation of <span class="math notranslate nohighlight">\(-\log Z(\theta)\)</span>. Rather than estimate only <span class="math notranslate nohighlight">\(\theta\)</span>, the NCE procedure treats c as just another parameter and estimate <span class="math notranslate nohighlight">\(\theta\)</span> and c simultaneously, using same algorithm for both. The resulting <span class="math notranslate nohighlight">\(\log p_{model}(x)\)</span> might not be correspond to exactly to a valid probability distribution, but it will become close and closer to being valid as the estimateof x improves.</p>
<p>NCE works by reducing the unsupervised learning problem of estimating p(x) to that of learning a probablistic binary classifier in which one of the categories corresponds to the data generated by the model. Specifically, we introduce a noise distribution <span class="math notranslate nohighlight">\(p_{noise}(x)\)</span> which is tractable to evaluate and sample from. We can now construct a model over both x and a new, binary class variable y. In the new joint model:</p>
<div class="math notranslate nohighlight">
\[\begin{split}p_{joint}(y=1) = \frac{1}{2} \\
p_{joint}(x|y=1) = p_{model}(x) \\
p_{joint}(x|y=0) = p_{noise}(x)\end{split}\]</div>
<p>y is a switch variable that determines whether we will generate x from the model or from the noise distribution.</p>
<p>We can construct a similar joint model of training data. In this case, the switch variable determines whether we draw x from the data or from the noise.</p>
<div class="math notranslate nohighlight">
\[\begin{split}p_{train}(y=1) = \frac{1}{2} \\
p_{train}(x|y=1) = p_{data}(x) \\
p_{train}(x|y=0) = p_{noise}(x)\end{split}\]</div>
<p>We can now just use standard maximum likehood learning on the supervised learning problem of fitting <span class="math notranslate nohighlight">\(p_{joint}\)</span> to <span class="math notranslate nohighlight">\(p_{train}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\theta, c = argmax_{\theta, c} E_{x, y \sim p_{train}} \log p_{joint}(y|x)\]</div>
<p><span class="math notranslate nohighlight">\(p_{joint}\)</span>: a logistic regression model applied to the difference in log probabilities of the model and the noise distribution:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin {equation}
\begin{split}
p_{joint}(y=1) &amp;= \frac{p_{model}(x)}{p_{model}(x) + p_{noise}(x)} \\
&amp;= \frac{1}{1 + exp(\log \frac{p_{noise}(x)}{p_{model}(x)})} \\
&amp;=\sigma(\log p_{model}(x) - \log p_{noise}(x))
\end{split}
\end {equation}\end{split}\]</div>
<p>NCE is simple to apply as long as</p>
<ul class="simple">
<li><span class="math notranslate nohighlight">\(\hat{p}_{model}\)</span> is easy to back-propagate</li>
<li><span class="math notranslate nohighlight">\(p_{noise}(x)\)</span> is easy to evaluate in order to evaluate <span class="math notranslate nohighlight">\(p_{joint}(x)\)</span></li>
<li><span class="math notranslate nohighlight">\(p_{noise}(x)\)</span> is easy to sample from to generate training data</li>
</ul>
<p>When to use Noise Contrastive Estimation:</p>
<ul class="simple">
<li>NCE most successful when applied to problem with few random variables, but it can work well even if those random variables can take on a high number of values. E.g: modeling the conditional distribution over a work given the context of the word.</li>
<li>Less efficient when applied to problem with many random variables. The logistic regression classifier can reject a noise sample by identifying any one variable whose value is unlikely. This means that learning slow greatly after <span class="math notranslate nohighlight">\(p_{model}\)</span> has learned the basic marginal statistics.</li>
</ul>
<p>The constraint that <span class="math notranslate nohighlight">\(p_{noise}\)</span> must be easy to evaluate and easy to sample from can be overly restrictive. When <span class="math notranslate nohighlight">\(p_{noise}\)</span> is too simple, most samples are likely to be too obviously distinct from the data to force <span class="math notranslate nohighlight">\(p_{model}\)</span> to improve noticeably.</p>
<p>Like score matching and pseudolikehood, NCE does not work if only a lower bound on <span class="math notranslate nohighlight">\(\hat{p}\)</span> is available.</p>
<p>When the model distribution is copied to define a new noise distribution before each gradient step, NCE defines a procedure called self-contrastive estimation, whose expected gradient is equivalent to the expected gradient of maximum likehood.</p>
<ul class="simple">
<li>The special case of NCE where the noise samples are those generated by the model suggests that maxium likehood can be interpreted as a procedure that forces a model to constantly learn to distinguish reality from its own evolving beliefs, while noise</li>
<li>Noise contrastive estimation achieves some reduced computational cost by only forcing the model to distinguish reality from a fixed baseline (the noise model).</li>
</ul>
<div class="section" id="resource">
<h2>Resource<a class="headerlink" href="#resource" title="Permalink to this headline">Â¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="https://stats.stackexchange.com/questions/244616/how-does-negative-sampling-work-in-word2vec/245452#245452">Intuitive explanation of Noise Contrastive Estimation loss</a></li>
<li><a class="reference external" href="http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf">Original Paper</a></li>
<li><a class="reference external" href="https://skymind.ai/wiki/word2vec">What is word2vec</a></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../Extra/index.html" class="btn btn-neutral float-right" title="Extra" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="18.5 Denoising Score Matching.html" class="btn btn-neutral" title="18.5 Denoising Score Matching" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Ximing

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>