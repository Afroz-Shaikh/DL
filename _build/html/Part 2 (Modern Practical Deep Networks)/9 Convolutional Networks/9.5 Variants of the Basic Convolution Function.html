

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>9.5 Variants of the Basic Convolution Function &mdash; dl 0.0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="9.6 Structured Output" href="9.6 Structured Output.html" />
    <link rel="prev" title="9.4 Convolution and Pooling as a Infinitely Strong Prior" href="9.4 Convolution and Pooling as a Infinitely Strong Prior.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> dl
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../Part 1 (Applied Math and Machine Learning Basics)/index.html">Part I: Applied Math and Machine Learning Basics</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Part II: Modern Practical Deep Networks</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../8 Optimization for Training Deep Models/index.html">8 Optimization for Training Deep Models</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">9 The convolutional Networks</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="9.1 The Convolutional Operation.html">9.1 The Convolutional Operation</a></li>
<li class="toctree-l3"><a class="reference internal" href="9.2 Motivation.html">9.2 Motivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="9.3 Pooling.html">9.3 Pooling</a></li>
<li class="toctree-l3"><a class="reference internal" href="9.4 Convolution and Pooling as a Infinitely Strong Prior.html">9.4 Convolution and Pooling as a Infinitely Strong Prior</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">9.5 Variants of the Basic Convolution Function</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#full-convolution">Full Convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#unshared-convolution">Unshared Convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tiled-convolution">Tiled Convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#back-prop-in-conv-layer">Back prop in conv layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bias-in-after-conv">Bias in after conv</a></li>
<li class="toctree-l4"><a class="reference internal" href="#resources">Resources</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="9.6 Structured Output.html">9.6 Structured Output</a></li>
<li class="toctree-l3"><a class="reference internal" href="9.7 Data Type.html">9.7 Data Type</a></li>
<li class="toctree-l3"><a class="reference internal" href="9.8 Efficient Convolution Algorithms.html">9.8 Efficient Convolution Algorithms</a></li>
<li class="toctree-l3"><a class="reference internal" href="9.9 Random or Unsupervised Features.html">9.9 Random or Unsupervised Features</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../11 Practical Methodology/index.html">11 Practical Methodoloogy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../12 Applications/index.html">12 Applications</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../Part 3 (Deep Learning Research)/index.html">Part III: Deep Learning Research</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Extra/index.html">Extra</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">dl</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Part II: Modern Practical Deep Networks</a> &raquo;</li>
        
          <li><a href="index.html">9 The convolutional Networks</a> &raquo;</li>
        
      <li>9.5 Variants of the Basic Convolution Function</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/Part 2 (Modern Practical Deep Networks)/9 Convolutional Networks/9.5 Variants of the Basic Convolution Function.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="variants-of-the-basic-convolution-function">
<h1>9.5 Variants of the Basic Convolution Function<a class="headerlink" href="#variants-of-the-basic-convolution-function" title="Permalink to this headline">¶</a></h1>
<p>Convolution in the context of NN means an operation that consists of many applications of convolution in parallel.</p>
<ul class="simple">
<li>Kenel K with element <span class="math notranslate nohighlight">\(K_{i, j, k, l}\)</span> giving the connection strength between a unit in channel i of output and a unit in channel j of the input, with an offset of k rows and l columns between the output unit and the input unit.</li>
<li>Input: <span class="math notranslate nohighlight">\(V_{i, j, k}\)</span> with channel i, row j and column k</li>
<li>Output Z same format as V</li>
<li>Use 1 as first entry</li>
</ul>
<div class="section" id="full-convolution">
<h2>Full Convolution<a class="headerlink" href="#full-convolution" title="Permalink to this headline">¶</a></h2>
<div class="section" id="padding-1-stride">
<h3>0 Padding 1 stride<a class="headerlink" href="#padding-1-stride" title="Permalink to this headline">¶</a></h3>
<div class="math notranslate nohighlight">
\[Z_{i, j, k} = \sum_{l, m, n} V_{l, j + m - 1, k + n - 1} K_{i, l, m, n}\]</div>
</div>
<div class="section" id="padding-s-stride">
<h3>0 Padding s stride<a class="headerlink" href="#padding-s-stride" title="Permalink to this headline">¶</a></h3>
<div class="math notranslate nohighlight">
\[Z_{i,j,k} = c(K, V, s)_{i, j, k} = \sum_{l, m, n}[V_{l, s * (j - 1) + m, s * (k - 1) + n} K_{i, l, m, n}]\]</div>
<p>Convolution with a stride greater than 1 pixel is equivalent to conv with 1 stride followed by downsampling:</p>
<img alt="../../_images/Figure9.12.PNG" src="../../_images/Figure9.12.PNG" />
</div>
<div class="section" id="some-0-paddings-and-1-stride">
<h3>Some 0 Paddings and 1 stride<a class="headerlink" href="#some-0-paddings-and-1-stride" title="Permalink to this headline">¶</a></h3>
<p>Without 0 paddings, the width of representation shrinks by one pixel less than the kernel width at each layer. We are forced to choose between shrinking the spatial extent of the network rapidly and using small kernel. 0 padding allows us to control the kernel width and the size of the output independently.</p>
<img alt="../../_images/Figure9.13.PNG" src="../../_images/Figure9.13.PNG" />
<p>Special case of 0 padding:</p>
<ul class="simple">
<li>Valid: no 0 padding is used. Limited number of layers.</li>
<li>Same: keep the size of the output to the size of input. Unlimited number of layers. Pixels near the border influence fewer output pixels than the input pixels near the center.</li>
<li>Full: Enough zeros are added for every pixels to be visited k (kenel width) times in each direction, resulting width m + k - 1. Difficult to learn a single kernel that performs well at all positions in the convolutional feature map.</li>
</ul>
<p>Usually the optimal amount of 0 padding lies somewhere between ‘Valid’ or ‘Same’</p>
</div>
</div>
<div class="section" id="unshared-convolution">
<h2>Unshared Convolution<a class="headerlink" href="#unshared-convolution" title="Permalink to this headline">¶</a></h2>
<p>In some case when we do not want to use convolution but want to use locally connected layer. We use <strong>Unshared convolution</strong>. Indices into weight W</p>
<ul class="simple">
<li>i: the output channel</li>
<li>j: the output row;</li>
<li>k: the output column</li>
<li>l: the input channel</li>
<li>m: row offset within input</li>
<li>n: column offset within input</li>
</ul>
<div class="math notranslate nohighlight">
\[Z_{i, j, k} = \sum_{l, m, n} [V_{l, i + m - 1, j + n - 1} W_{i, j, k, l, m, n}]\]</div>
<p>Comparison on local connections, convolution and full connection</p>
<img alt="../../_images/Figure9.14.PNG" src="../../_images/Figure9.14.PNG" />
<p>Useful when we know that each feature should be a function of a small part of space, but no reason to think that the same feature should occur accross all the space. eg: look for mouth only in the bottom half of the image.</p>
<p>It can be also useful to make versions of convolution or local connected layers in which the connectivity is further restricted, eg: constrain each output channeel i to be a function of onlu a subset of the input channel. Adv:</p>
<ul class="simple">
<li>reduce memory consumption</li>
<li>increase statistical efficiency</li>
<li>reduce computation for both forward and backward prop.</li>
</ul>
<img alt="../../_images/Figure9.15.PNG" src="../../_images/Figure9.15.PNG" />
</div>
<div class="section" id="tiled-convolution">
<h2>Tiled Convolution<a class="headerlink" href="#tiled-convolution" title="Permalink to this headline">¶</a></h2>
<p>Learn a set of kernels that we rotate through as we move through space. Immediately neighboring locations will have different filters, but the memory requirement for storing the parameters will increase by a factor of the size of this set of kernels. Comparison on locally connected layers, tiled convolution and stardard convolution:</p>
<img alt="../../_images/Figure9.16.PNG" src="../../_images/Figure9.16.PNG" />
<p>K: 6-D tensor, t different choice of kernel stack</p>
<div class="math notranslate nohighlight">
\[Z_{i, j, k} = \sum_{l, m, n}[V_{i, i + m - 1, j + n - 1}K_{i, l, m, n, j \% t + 1, k \% t + 1}]\]</div>
<p>Local connected layers and tiled convolutional layer with max pooling: the detector units of these layers are driven by different filters. If the filters learn to detect different tranformed version of the same underlying features, then the max-pooled units become invariant to the learned transformation.</p>
<p>Review:</p>
<img alt="../../_images/Figure9.7.PNG" src="../../_images/Figure9.7.PNG" />
<img alt="../../_images/Figure9.9.PNG" src="../../_images/Figure9.9.PNG" />
</div>
<div class="section" id="back-prop-in-conv-layer">
<h2>Back prop in conv layer<a class="headerlink" href="#back-prop-in-conv-layer" title="Permalink to this headline">¶</a></h2>
<p>Back prop of conv layer:</p>
<ul class="simple">
<li>K: Kernel stack</li>
<li>V: Input image</li>
<li>Z: Output of conv layer</li>
<li>G: gradient on Z</li>
</ul>
<img alt="../../_images/ConvBackProp.jpg" src="../../_images/ConvBackProp.jpg" />
</div>
<div class="section" id="bias-in-after-conv">
<h2>Bias in after conv<a class="headerlink" href="#bias-in-after-conv" title="Permalink to this headline">¶</a></h2>
<p>We generally add some bias term to each output before applying nonelinearity.</p>
<ul class="simple">
<li>For local conncted layers: give each unit its one bias</li>
<li>For tiled conv layers: share the biases with the same tiling pattern as the kernels</li>
<li>For conv layers: have one bias per channel of the output and share it accross all locations within each convolution map. If the input is fixed size, it is also possible to learn a seperate bias at each location of the output map.</li>
</ul>
</div>
<div class="section" id="resources">
<h2>Resources<a class="headerlink" href="#resources" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="http://cs231n.github.io/convolutional-networks/#convert">Converting FC to CONV Layer</a></li>
<li><a class="reference external" href="http://www.iro.umontreal.ca/~lisa/pointeurs/convolution.pdf">Technical Report Multidimensional Downsampled Convolution for Autoencoders</a></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="9.6 Structured Output.html" class="btn btn-neutral float-right" title="9.6 Structured Output" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="9.4 Convolution and Pooling as a Infinitely Strong Prior.html" class="btn btn-neutral" title="9.4 Convolution and Pooling as a Infinitely Strong Prior" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Ximing

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>