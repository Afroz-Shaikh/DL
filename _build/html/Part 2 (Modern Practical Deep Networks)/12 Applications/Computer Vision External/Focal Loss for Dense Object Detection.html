

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Focal Loss for Dense Object Detection &mdash; dl 0.0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Part III: Deep Learning Research" href="../../../Part 3 (Deep Learning Research)/index.html" />
    <link rel="prev" title="YOLO 2" href="YOLO 2.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html" class="icon icon-home"> dl
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../Part 1 (Applied Math and Machine Learning Basics)/index.html">Part I: Applied Math and Machine Learning Basics</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html">Part II: Modern Practical Deep Networks</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../8 Optimization for Training Deep Models/index.html">8 Optimization for Training Deep Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../9 Convolutional Networks/index.html">9 The convolutional Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../11 Practical Methodology/index.html">11 Practical Methodoloogy</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html">12 Applications</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="index.html">External Resources On Computer Vision</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="YOLO.html">YOLO</a></li>
<li class="toctree-l4"><a class="reference internal" href="YOLO 2.html">YOLO 2</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">Focal Loss for Dense Object Detection</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../Part 3 (Deep Learning Research)/index.html">Part III: Deep Learning Research</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Extra/index.html">Extra</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">dl</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Part II: Modern Practical Deep Networks</a> &raquo;</li>
        
          <li><a href="../index.html">12 Applications</a> &raquo;</li>
        
          <li><a href="index.html">External Resources On Computer Vision</a> &raquo;</li>
        
      <li>Focal Loss for Dense Object Detection</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/Part 2 (Modern Practical Deep Networks)/12 Applications/Computer Vision External/Focal Loss for Dense Object Detection.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="focal-loss-for-dense-object-detection">
<h1>Focal Loss for Dense Object Detection<a class="headerlink" href="#focal-loss-for-dense-object-detection" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>1. Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Current state of art object detector: R-CNN framewor. Process</p>
<ol class="arabic simple">
<li>Generates a sparse set of condidate object locations</li>
<li>Classifies each condidate location as one of the foreground classes or as background</li>
</ol>
<p>This 2-stage framework consitently achieves top <strong>accuracy</strong> on the challenging COCO benchmark.</p>
<p>One stage detector: YOLO, SSD. Faster with accuracy within 10-40% relative to state of art two-stage methods.</p>
<p>This paper: one-stage object detector, matches state of art COCO AP of more complex 2-stage detector. To achive this result, we identify imbalance during training as the main obstacle impeding 1-stage detector and propose a new loss function that eliminates this barrier.</p>
<ul>
<li><p class="first">Class imbalance is addressed in R-CNN-like detectors by 2-stage cascade and sampling heuristics.</p>
<blockquote>
<div><ol class="arabic simple">
<li>The proposal stage rapidly narrows down the number of candidate object locatios to a small number (e.e 1-2k).</li>
<li>In the second classification stage, sampling heuristics are performed to maintain a managable balance between foreground and background.</li>
</ol>
</div></blockquote>
</li>
<li><p class="first">1-stage detector must process a much larger set of canidate object location regularly sampled accross an image (~ 100k locations). While similar sampling heuristics may also be applied, they are inefficient as the training procedure is still dominated by easily classifid background examples. This inefficiency is typically addressed via techniques such as bootstrapping or hard example mining.</p>
</li>
<li><p class="first">This paper, we propose a new loss function that act as a more effective alternative to previous approaches for dealing with class imbalance. Loss function is dynamically scaled cross entropy loss, where the scaling factor decays to 0 as confidence in the correct class increases.</p>
</li>
</ul>
<img alt="../../../_images/RetinaNetFigure1.PNG" src="../../../_images/RetinaNetFigure1.PNG" />
<p>This scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard exampls. Finally we note that the exact form of the focal loss is not crucial, and we show other instantials can achive similar results.</p>
</div>
<div class="section" id="related-work">
<h2>2. Related work<a class="headerlink" href="#related-work" title="Permalink to this headline">¶</a></h2>
<ul>
<li><p class="first">Classic object detectors: sliding window paradigm, in which a classifier is applied on dense image grid.</p>
</li>
<li><p class="first">2-stage detectors</p>
<blockquote>
<div><ol class="arabic simple">
<li>generates a sparse set of condidate proposals that should contain all objects while filtering out the majority of negative locations</li>
<li>classify the proposal into forground classes/background</li>
</ol>
</div></blockquote>
</li>
<li><p class="first">1-stage detectors</p>
<blockquote>
<div><ul class="simple">
<li>Tuned for speed but accuracy trails that of two-stage method</li>
<li>2-stage detectors can be made fast simply by reducing image resolution and number of proposal</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">RetinaNet</p>
<blockquote>
<div><ul class="simple">
<li>shares many similarities with previous dense detector, such as anchor in RPN, features pyramids in SSD and FPN</li>
<li>simple detector achieves top results not based on innovations in design but due to novel loss.</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>Classic 1-stage object detection methods, and more recent methods face a large class imbalance during training. These detector evaluate <span class="math notranslate nohighlight">\(10^4 - 10^5\)</span> candidate locations per image but only a few location contain objects. Cause 2 problems</p>
<ol class="arabic simple">
<li>Training is inefficient as most locations are easy negatives that contributes no useful learning signal.</li>
<li>The easy negatives can overwhelm training and lead to degenerate models.</li>
</ol>
<div class="line-block">
<div class="line">Common solution: hard negative mining that samples hard examples during training or more complex sampling/reweighing schemes.</div>
<div class="line">Proposal: focal loss natually handles the class imbalance faced by one stage detector and allow us to efficiently train on all examples without sampling and without easy negatives overwhelming the loss and computed gradients.</div>
</div>
<ul class="simple">
<li>Robust loss: down-weighting the loss of examples with large error (hard examples).</li>
<li>Focal loss: down-weighting the loss of easy examples. It focus on training in a sparse set of hard examples.</li>
</ul>
</div>
<div class="section" id="focal-loss">
<h2>3. Focal Loss<a class="headerlink" href="#focal-loss" title="Permalink to this headline">¶</a></h2>
<p>I am not using the example of binary class in the paper, instead, I found a great explanation tutorial from <a class="reference external" href="https://towardsdatascience.com/retinanet-how-focal-loss-fixes-single-shot-detection-cb320e3bb0de">here</a></p>
<p>Cross Entropy:</p>
<div class="math notranslate nohighlight">
\[CE = - \sum y_i(log P_i)\]</div>
<p>Where <span class="math notranslate nohighlight">\(y_i = 1\)</span> if the training example belongs to class i (<span class="math notranslate nohighlight">\(\vec{y}\)</span> is an one-hot vector), <span class="math notranslate nohighlight">\(P_i\)</span> is the predicted probability. Even examples that are easily classified with <span class="math notranslate nohighlight">\(P_i &gt;&gt; 0.5\)</span> incur a loss with non-trivial magnitude. Because, even though the contribution from a single example to the loss is small, the total contribution from a large amount of examples can be non-trivial. When summed over a large number of easy examples, these small loss values can be overwhelm the rare class.</p>
<div class="section" id="focal-loss-definition">
<h3>3.2 Focal Loss Definition<a class="headerlink" href="#focal-loss-definition" title="Permalink to this headline">¶</a></h3>
<div class="math notranslate nohighlight">
\[C(p, y) = - \sum y_i(1 - p_i)^\gamma \log p_i\]</div>
<p><span class="math notranslate nohighlight">\((1 - p_i)\)</span>: Modulating factor. <span class="math notranslate nohighlight">\(\gamma\)</span> focusing parameter.</p>
<ul class="simple">
<li>When the class is misclassified and <span class="math notranslate nohighlight">\(p_i\)</span> is small, the modulating factor is close to 1, the loss is unaffected. As <span class="math notranslate nohighlight">\(p_i \to 1\)</span>, the factor goes to 0, the well-classified examples is down-weighted.</li>
<li>The focusing paramter <span class="math notranslate nohighlight">\(\gamma\)</span> smoothly adjusts the rate at which easy examples are down weighted. When <span class="math notranslate nohighlight">\(\gamma = 0\)</span>, FL is equivalent to cross entropy. As <span class="math notranslate nohighlight">\(\gamma\)</span> is increased the effect of the modulating fatcor is likewise increased.</li>
</ul>
<p>In practice, we use <span class="math notranslate nohighlight">\(\alpha-balanced\)</span> varient of the focal loss</p>
<div class="math notranslate nohighlight">
\[C(p, y) = - \sum y_i \alpha_i (1 - p_i)^\gamma \log p_i\]</div>
<p>Slightly improved accuracy over the <span class="math notranslate nohighlight">\(non-\alpha-balanced\)</span> form. Implementation of the loss layer combines the sigmoid operation for computing p with the loss computation, resulting in greater numerical stability.</p>
</div>
<div class="section" id="class-imbalance-and-model-initialization">
<h3>3.3 Class Imbalance and Model Initialization<a class="headerlink" href="#class-imbalance-and-model-initialization" title="Permalink to this headline">¶</a></h3>
<p>Problem: initialize equal probability to all classes, the loss due to the fequent class can domintate total loss and cause instability in the early training.
Solution: introduce “prior” for the value of p etimated by the model for the rare class at the start of training. We denote the prior by <span class="math notranslate nohighlight">\(\pi\)</span> and set it so that the model’s estimated p for examples of the rare class is low. This improve training stability for both the cross entropy and focal loss in the case of heavy class imbalance.</p>
</div>
<div class="section" id="class-imbalance-and-2-stage-detectors">
<h3>3.4 Class Imbalance and 2-stage detectors<a class="headerlink" href="#class-imbalance-and-2-stage-detectors" title="Permalink to this headline">¶</a></h3>
<p>How 2-stage detector address class imbalance:</p>
<ul>
<li><p class="first">cascase</p>
<blockquote>
<div><ul class="simple">
<li>reduce nearly infinite set of possible object locations down to 1000 or 2000.</li>
<li>not random, likely to correspond to true object locations, which removes vast majority of easy negatives.</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">biased minibatch</p>
<blockquote>
<div><ul class="simple">
<li>construct mini-batch that contains, for instance, 1:3 ratio of positive and negative examples. The ratio is like an implicit <span class="math notranslate nohighlight">\(\alpha-balance\)</span> factor that is implemented via sampling.</li>
</ul>
</div></blockquote>
</li>
</ul>
</div>
</div>
<div class="section" id="retinanet-detector">
<h2>4. RetinaNet Detector<a class="headerlink" href="#retinanet-detector" title="Permalink to this headline">¶</a></h2>
<ul>
<li><p class="first">One backbone network: compute convolutional feature map over an entire image, off-the-shelf conv net.</p>
</li>
<li><p class="first">Two task specific subnetworks</p>
<blockquote>
<div><ol class="arabic simple">
<li>Convolutional object classification on the backbone’s output</li>
<li>Bounding box regression</li>
</ol>
</div></blockquote>
</li>
</ul>
<img alt="../../../_images/RetinaNetFigure3.PNG" src="../../../_images/RetinaNetFigure3.PNG" />
<p>Feature Pyramid Network (FPN) Backbone: augments a std convolutional network with a top-dwon pathway and lateral connections so the network efficiently construcs a rich, multi-scale feature pyramid from a single resolution imput image. Each level of the pyramid can be used for detecting objectes at a different scale. The use of FPN backbone is preliminary; experience using features from only the final ResNet layer yield low AP.</p>
<p><a class="reference external" href="https://www.coursera.org/lecture/convolutional-neural-networks/anchor-boxes-yNwO0">Anchors</a> : In total there are A = 9 anchors per level and cross levels they cover the scale range 32 - 813 pixels with respect to the network input image. Each anchor is assigned a length K one-hot vector of classification targets where K is the number of object classes, and a 4-vector of box regression targets. Anchors are assigned to ground-truth object boxes using an IoU threshold of 0.5 and to background if their IoU is in [0, 0.4). As each anchor is assigned to at most one object box, we set the corresponding entry in its length K label vector to 1 and all other entries to 0. If an anchor is unassigned, which may happen with IoY in [0.4, 0.5), it is ignored during training. Box regression targets are compued as the offset between each anchor and its assigned object box or omitted if there is no assignment.</p>
<p>Classification Subnets: predicts the probability of object presence at each spatial position for each of the A anchors and K object classes. This subnet is a small FCN attatched to each FPN level. parameters of this subnet are shared accross all pyramid levels. Taking an input feature map with C channels from a given pyramid level, the subnet applies 4 3 * 3 conv layers, each with C filters and each followed by RelU, followed by a 3 * 3 conv layer with KA filters. Finally sigmoid activations are attached to output the KA binary predictions per spatial location.</p>
<p>Box regression subnet: In parallel with the object classification subnet, we attatch another small FCN to each pyramid level for the purpose of regression the offset from each anchor box to a nearby ground-truth object. The design of the box regression subnet is identical to the classification subnet except that it ternubates in 4A linear outputs per spatial location, these 4 output predict the relatibe offset between the anchor and the ground truth box</p>
</div>
<div class="section" id="inference-and-training">
<h2>4.1 Inference and training<a class="headerlink" href="#inference-and-training" title="Permalink to this headline">¶</a></h2>
<p>Inference: Inference involves simply forwarding an image through the network. To improve speed, we only decode box predictions from at most 1k top scoring prediction per FPN level, after thresholding detector confidence at 0.05. The top predictions from all levels are merged and non-maximim suppression with a threshold of 0.5 is applied to yield the final detections.</p>
<p>Focal loss: it is applied to all ~100k anchors in each sampled image. The total focal loss of an image is computed as the sum of the focal loss over all ~100k anchors, normalized by the number of anchors assigned to a ground truth box. Reason: vast majority of anchors are easy negatives and receive negligible loss value value under the focal loss. In general, <span class="math notranslate nohighlight">\(\alpha\)</span> should be decreased slightly as <span class="math notranslate nohighlight">\(\gamma\)</span> is increased.</p>
<p>Initialization: pretrained on ImageNet 1k. All new conv layers except the final one in the RetinaNet subnets are intialized with bias b=0 and a Gaussian weight fill with <span class="math notranslate nohighlight">\(\sigma=0.01\)</span>. For the final layer of classification subnet, we set the bias initialization to be <span class="math notranslate nohighlight">\(b = -log((1-\pi)/\pi)\)</span>, where <span class="math notranslate nohighlight">\(\pi\)</span> the start of training every anchor should be labeled as foreground with confidence of <span class="math notranslate nohighlight">\(\pi\)</span>. This init prevents the large number of background anchors from generating a large, destabilizing loss value in the first iteration of training.</p>
</div>
<div class="section" id="external-resources">
<h2>External Resources<a class="headerlink" href="#external-resources" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="https://towardsdatascience.com/retinanet-how-focal-loss-fixes-single-shot-detection-cb320e3bb0de">RetinaNet how Focal Loss fixes Single Shot Detection</a></li>
<li><a class="reference external" href="https://www.coursera.org/lecture/convolutional-neural-networks/anchor-boxes-yNwO0">Anchor Box</a></li>
<li><a class="reference external" href="https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/">Faster RCNN Down the rabbit hole of modern object detection</a></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../../Part 3 (Deep Learning Research)/index.html" class="btn btn-neutral float-right" title="Part III: Deep Learning Research" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="YOLO 2.html" class="btn btn-neutral" title="YOLO 2" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Ximing

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>