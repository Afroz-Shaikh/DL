Paper: Auto-Encoding Variational Bayes
======================================

`Paper <https://arxiv.org/pdf/1312.6114.pdf>`_

How to perform efficient approximate inference and learning with directed probablistic models whose continuous latent variable and/or parameters have intractable posterior. 

Variational Bayesian involves the optimization of an approximation to the intractable posterior. The common Mean field approach requires analystical solution of expectation w.r.t the approximate posterior, which are also intractable in general case. 

This paper shows how a reparameterization of variational lower bound yields a simple differentiable unbiased estimator of the lower bound. 

This Stochastic Gradient Variational Bayesian can be used for effiecient approximate posterior inference in almost any model with continuous latent variable and/or parameters. And it is straightforward to optimize using standard stochastic gradient ascent technique. 

For the case of i.i.d dataset and continuous latent variables per datapoint, the paper propose autoencoder VB. It uses SGVB estimator to optimize the recognition model that allow us to perform efficient approximate posterior inference using simple ancestral sampling => make inference and learning especially effcient, which in turn to allow us to efficiently learn the model parameters. 

Learned posterior inference model, can also be used for a host of tasks

* recognition
* denoising 
* representation
* visualization

When a neural network is used for the recognition model, we reach variational auto-encoder.


############################################################
2 Method
############################################################

This paper, we restrict the ourselves to the common case where 

* We have an i.i.d dataset with latent variable per datapoint, 
* We like to perform maximum likehood or maximum a posteriori inference on the global variables and variational inference on the latent variables

We can extend the scenario if we want. But the paper is focused on the common cases.

************************************************************
2.1 Problem scenario
************************************************************

review on representation learning in DL textbook at the beginning of chapter 15:

Feedforward network trained by supervised learning: last layer linear classifier or nearest neighbor classifier. The rest of the network learns to provide a representation to this classifier. Training with a supervised criterion naturally leads to the representatino at every hidden layer taking on properties that make the classification easier.

Just like superviser network, unsupervised deep learning algorithms have a main training objective but also learn a representation as a side effect. Most of representation learning problem face the trade off between

* Researve as much info about input as possible
* attain nice properties such as independence

Consider some dataset: :math:`X = {x^{i}}_{i=1}^N` consisting of N i.i.d samples of some discrete or continuous variable x. 

We assume that the data are generated by some random random process, involving an unobserved continuous random variable z. This data generating processs consists of 2 step:

1. a value :math:`z^{(i)}` is generated from some prior distribution :math:`p_{\theta^*}(z)`.
2. a value :math:`x^{(i)}` is generated from some conditional distribution :math:`p_{\theta^*}(x|z)`. We assume that 
	
	a. the prior :math:`p_{\theta^*}(z)` and the likehood :math:`p_{\theta^*}(x|z)` come from parametric families of :math:`p_{\theta}(z)` and :math:`p_{\theta}(x|z)`
	b. Their probability density functions (PDFs) are differentiable with respect to both :math:`\theta` and z

Unfortunately, a lot of this process is hidden from our view: the true parameters :math:`\theta^*` as well as the values of the latent variable :math:`z^{(i)}` are unknown to us.

Very importantly, we do not make simplifying assumption about

a. the marginal :math:`p_{\theta}(x)`
b. the posterior :math:`p_{\theta}(z|x)`

This kind of assumption is very common to solve the challenge. But no! we are not going to use this strategy. So without this kind of simplying assumption, what kind of challenge are we facing: 

1. Intractibility: 
	
	a. marginal likehood :math:`p_{\theta}(x) = \int p_{\theta}(z) p_{\theta}(x|z) dz` is intractible
	b. true posterior :math:`p_{\theta}(z|x) = p_{\theta}(x|z) * p_{\theta}(z) / p_{\theta}(x)` is intractible
	c. required integrals integrals for any reasonable mean-field VB algorithm are also intractable. 

2. A large dataset: we have so much data that batch optimization is too costly. We would like to make parameter updates using small minibatch or even single datapoints. Sampling based solution, e.g. Monte Carlo, EM would in general be too slow, since it involves a typically expensive sampling loop per datapoint. 

Review on intractable inference problem at Chapter 19 of DL textbook:

.. image:: rsc/Figure19.1.PNG

We are intersted in and propose solution to, 3 related problems in the above scenario:

1. Efficient approximate maximum likelihood (ML) or maximum a posteriori (MAP) estimation for the parameter :math:`\theta`. 
2. Efficient approximate posterior inference of the latent variable z given an observed value of x for a choice of :math:`\theta`. This is useful for coding and data representation tasks.
3. Efficient approximate marginal inference of the variable x. This allow us to perform all kinds of tasks where a prior over x is required. Common applications in computer vision include image denoising, inpainting, and super resolution. 

For the purpose of solving the above problems, we introduce a recognition model :math:`q_{\phi}(z|x)`: an approximation to the **intractable true posterior** :math:`p_{\theta}(z|x)`

* :math:`q_{\phi}(z|x)` probabslistic encoder: given datapoint x, it produces a distribution (e.g. a Gaussian) over the possible values of code z from which the datapoint x could have been generated. 
*  :math:`p_{\theta}(x|z)` probabslistic decoder: given a code z, it produces a distribution over the possible corresponding value of x. 

************************************************************
2.2 The variational bound
************************************************************

.. math::

	\begin {equation}
	\begin{split}
	KL[q_{\phi}(z|x) || p_{\theta}(z|x)] &= \sum_h q_{\phi}(z|x) \log \frac{q_{\phi}(z|x)}{p_{\theta}(z|x)} \\ \\
	&= \sum_h q_{\phi}(z|x) \log \frac{q_{\phi}(z|x)}{\frac {p_{\theta}(x, z)}{p_{\theta}(x)}} \\ \\
	&= \log p_{\theta}(x) + \sum_z q_{\phi}(z|x) (\log q_{\phi}(z|x) - log p_{\theta}(x, z) \\ \\
	&=  \log p_{\theta}(x) + \sum_z q_{\phi}(z|x) (\log q_{\phi}(z|x) - log p_{\theta}(x | z) p_{\theta}(z) \\ \\
	&= \log p_{\theta}(x) + KL[q_{\phi}(z|x) || p_{\theta}(z)] - \sum_z q_{\phi}(z|x) log p_{\theta}(x | z)
	\end{split}
	\end {equation}


Now we have 

.. math::
	
	\log p_{\theta}(x) = KL[q_{\phi}(z|x) || p_{\theta}(z|x)] - KL[q_{\phi}(z|x) || p_{\theta}(z)] + \sum_z q_{\phi}(z|x) log p_{\theta}(x | z)

Because :math:`KL[q_{\phi}(z|x) || p_{\theta}(z|x)] >= 0` we have 

.. math::
	\begin {equation}
	\begin{split}
	\log p_{\theta}(x) &\geq L(\theta, \phi; x) \\ \\
	&= - KL[q_{\phi}(z|x) || p_{\theta}(z)] + \sum_z q_{\phi}(z|x) log p_{\theta}(x | z) \\ \\
	&= - KL[q_{\phi}(z|x) || p_{\theta}(z)] + E_{z \sim q_{\phi}(z|x)} log p_{\theta}(x | z)
	\end{split}
	\end {equation}

we want to differentiate and optimize lower bound w.r.t. both variational parameter :math:`\phi` and generative parameters :math:`\theta`. 

To calculate a function w.r.t. :math:`\phi` using naive / Monte Carlo method we have:

.. math::
	\begin {equation}
	\begin{split}
	\nabla_{\phi} E_{z \sim q_{\phi}(z)} f(z) &= \sum_z \nabla_{\phi} q_{\phi}(z)f(z) \\ \\
	&= \sum_z f(z) \nabla_{\phi} q_{\phi}(z) \\ \\
	&= \sum_z f(z) q_{\phi}(z) \nabla_{\phi} \log q_{\phi}(z) \\ \\
	&= E_{z \sim q_{\phi}(z)} [f(z)  \nabla_{\phi} \log q_{\phi}(z)] \\ \\ 
	&\approx \frac{1}{L}\sum_{l=1}^L [f(z^{(l)})  \nabla_{\phi} \log q_{\phi}(z^{(l)})] \\ \\
	\end{split}
	\end {equation}

where :math:`z^{(l)} \sim q_{\phi}(z)`. 

This gradient estimator exhibits very high variance.








